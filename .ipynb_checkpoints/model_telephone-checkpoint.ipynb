{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baf324bb",
   "metadata": {},
   "source": [
    "# Experimental Setup\n",
    "\n",
    "In this experiment I'll be using the [Stable Diffusion 2](https://huggingface.co/stabilityai/stable-diffusion-2) model to generate an image and the [Blip Large](https://huggingface.co/Salesforce/blip-image-captioning-large) model to caption to the images, and then using the caption as the next input prompt to Stable Diffusion. Ill then run this for N cycles, looking at the semantic decay that occurs over all and between cycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfc58ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant packages\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\n",
    "import torch\n",
    "from PIL import Image\n",
    "import transformers\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bdaf3b",
   "metadata": {},
   "source": [
    "### starting with some Transformers weirdness\n",
    "\n",
    "From the [docs](https://huggingface.co/docs/diffusers/optimization/mps): \"We recommend to “prime” the pipeline using an additional one-time pass through it. This is a temporary workaround for a weird issue we have detected: the first inference pass produces slightly different results than subsequent ones. You only need to do this pass once, and it’s ok to use just one inference step and discard the result.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29f215cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aa3611a30014a29b4ecd4080a1af2c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n",
      "/Users/iansotnek/miniforge3/envs/mldev/lib/python3.10/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08707a0bf66d4ca89730ad648d78eb3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iansotnek/miniforge3/envs/mldev/lib/python3.10/site-packages/diffusers/schedulers/scheduling_euler_discrete.py:168: UserWarning: The operator 'aten::nonzero' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1666646835428/work/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  step_index = (self.timesteps == timestep).nonzero().item()\n"
     ]
    }
   ],
   "source": [
    "#set up the stable diffusion pipeline\n",
    "\n",
    "diffusion_model_id = 'stabilityai/stable-diffusion-2'\n",
    "\n",
    "scheduler = EulerDiscreteScheduler.from_pretrained(diffusion_model_id, subfolder = 'scheduler')\n",
    "\n",
    "sd_pipe = StableDiffusionPipeline.from_pretrained(diffusion_model_id, scheduler = scheduler)\n",
    "\n",
    "sd_pipe.to('mps')\n",
    "\n",
    "sd_pipe.enable_attention_slicing()\n",
    "\n",
    "\n",
    "#warm-up prompt\n",
    "\n",
    "initial_prompt = 'An oil painting of a pirate ship made of Swiss cheese'\n",
    "\n",
    "_ = sd_pipe(initial_prompt, num_inference_steps = 1) \n",
    "\n",
    "#warmup_image = sd_pipe(initial_prompt).images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08496951",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the experiment         \n",
    "if not os.path.exists('images'):\n",
    "    os.mkdir(images)\n",
    "    \n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "    \n",
    "cycles = 10\n",
    "\n",
    "first_real_prompt = 'A brown and white corgi is eating a large watermelon while sitting on a towel at the beach'\n",
    "\n",
    "prompts = np.array([first_real_prompt])\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "captioning_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4803c081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9de7a961b194c38b00bedaf2d11c710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected one of cpu, cuda, ipu, xpu, mkldnn, opengl, opencl, ideep, hip, ve, fpga, ort, xla, lazy, vulkan, mps, meta, hpu, privateuseone device type at start of device string: mpl",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages/image_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpeg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m image\u001b[38;5;241m.\u001b[39msave(image_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJPEG\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m caption_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmpl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m caption \u001b[38;5;241m=\u001b[39m captioning_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcaption_inputs)\n\u001b[1;32m     11\u001b[0m prompts \u001b[38;5;241m=\u001b[39m prompts\u001b[38;5;241m.\u001b[39mappend([caption])\n",
      "File \u001b[0;32m~/miniforge3/envs/mldev/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:214\u001b[0m, in \u001b[0;36mBatchFeature.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;66;03m# check if v is a floating point\u001b[39;00m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_floating_point(v):\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;66;03m# cast and send to device\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m         new_data[k] \u001b[38;5;241m=\u001b[39m \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m         new_data[k] \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected one of cpu, cuda, ipu, xpu, mkldnn, opengl, opencl, ideep, hip, ve, fpga, ort, xla, lazy, vulkan, mps, meta, hpu, privateuseone device type at start of device string: mpl"
     ]
    }
   ],
   "source": [
    "for i in range(cycles):\n",
    "    prompt = prompts[i]\n",
    "    image = sd_pipe(prompt).images[0]\n",
    "    \n",
    "    image_path = f'images/image_{i}.jpeg'\n",
    "    image.save(image_path, 'JPEG')\n",
    "    \n",
    "    raw_image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    caption_inputs = processor(raw_image, return_tensors = 'pt')\n",
    "        \n",
    "    blip_out = captioning_model.generate(**caption_inputs)\n",
    "    \n",
    "    caption = processor.decode(blip_out[0], skip_special_tokens = True)\n",
    "    \n",
    "    prompts = np.append(prompts, [caption])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e25a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(prompts, columns = ['prompts'])\n",
    "data.to_csv('data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
